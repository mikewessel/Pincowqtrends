---
title: "SKTTrends"
author: "Mwessel"
date: "2023-04-12"
output: html_document
editor_options: 
  chunk_output_type: console
---

#*******************************************************************************************;
#
#                   JANICKI ENVIRONMENTAL, INC.
#
#
#
# Project        :  Pinellas County
#
# Program name   :  Step 2 PC Trend Tests
#
# Date           :  08/11/21
#
# Description    :   Conducts seasonal mann kendall trend tests on PC fixed and random data
#
# Input datasets :   Various
#
# Output datasets:   PC_trends_full_join
#
# Programmer(s)  :   Mike Wessel
#
#
#*******************************************************************************************;
#
# History:
#
#
#*******************************************************************************************



```{r setup, include=FALSE, echo=FALSE}
opts_knit$set(root.dir = "~/path/to/folder/")

# See library statements below for needed packages that need to be installed if not already
# e.g.  install.packages("rkt",dependencies=TRUE) # package to perform seasonal kendall tau;

# open libraries
library(tidyverse)
library(haven)
library(rkt)
library(here)
library(forecast)
library(pracma)
library(xts)
library(lmtest)
library(EnvStats)
library(imputeTS)
library(car)
library(foreign)
library(wqtrends)
library(lubridate)

# get working directory
setwd(here::here("./data"))

### CLEAR WORKSPACE - EACH RUN REQUIRES CLEARING CACHE TO RESET LISTS ETC..
rm(list = ls())

### Read in Location File
location<-read_sas("pinco_stations_watercourse2019.sas7bdat")%>%
  mutate(final_stat = as.factor(STATION))

```


Initial data analysis has led to selection of the following stations for analysis which include
probabilistic sampling in estuarine segments in western Pinellas County, Lake Seminole and select fixed stations. 


```{r getdata, include=FALSE, echo=FALSE}

# Pull some stations for analysis;
stats<-c("W1","W2","W3","W4","W5","W6","W7","W8","08-03","19-08","SA","SB","09-02","15-04","17-03")


## Wrangle

#skt trend set up - by sample period (i.e. mon)
forskt<-read_sas("pinellas_trend_input_2003_2020.sas7bdat")%>%
       mutate(mon=sample_period, station = as.factor(station),lvl=as.factor(Level), yr=Year,               value=Value,date = as.Date(Date), Station = station)%>%
   filter(lvl =="Surface" & Param %in% c("TN","Chl_a") & Station %in% stats)%>% 
    arrange(Station,Param,yr,mon,lvl)%>%
    group_by(Station,Param,yr,mon,lvl)%>%
    summarize(value=median(value,na.action=na.pass))%>%
  droplevels()
saveRDS(forskt,"forskt.RDS")

formt<-read_sas("pinellas_trend_input_2003_2020.sas7bdat")%>%
     mutate(station = as.factor(station),lvl=as.factor(Level), yr=Year,value=Value,
     date = as.Date(Date), param=Param,doy = yday(date), cont_year = decimal_date(date),segment="Dummy")%>%
     filter(lvl =="Surface" & Param %in% c("TN","Chl_a") & station %in% stats)%>% 
     na.omit() %>%  #tosses missing
    arrange(segment,station,param,date)%>%  
     droplevels()  
saveRDS(formt,"formt.RDS")
  
formt_raw<-read_sas("formt_raw.sas7bdat")%>%
    select(-COL2)%>%
     mutate(station = as.factor(station),lvl=as.factor(Level),date =                                as.Date(Date),yr=year(date),value=Value,param=Param,doy = yday(date), cont_year =              decimal_date(date),segment="Dummy")%>%
     filter(lvl =="Surface" & Param %in% c("TN","Chl_a") & station %in% stats)%>% 
     na.omit() %>%  #tosses missing
     arrange(segment,station,param,date)%>%  
     droplevels()
saveRDS(formt_raw,"formt_raw.RDS")

```

#####################################################################################
##  Raw Values
#####################################################################################

# below throws warnings but seems to run.NOTE Viewing tbtnmod takes a while to pop up.
formt_raw<-readRDS("formt_raw.RDS")

tbtnmod_raw <- formt_raw %>% 
    group_by(segment,station,param)%>% 
  nest() %>% 
  mutate(
    mod = purrr::pmap(list(station,param,data), function(station,param, data){
            cat(station, '\n')
            anlz_gam(data, trans = 'log10')
          })
  )%>%
  ungroup()

## from figs.r
modcmp_raw <- tbtnmod_raw%>%
  mutate(
    trn = purrr::pmap(list(station,param, mod, data), function(station,param, mod, data){

      cat(station, 'all \n')

      tomod <- data %>%
        filter(yr > 2002) %>%
        mutate(
          mo = month(date)
        )

# run meta test
      coefs1 <- anlz_metseason(mod, metfun = mean, doystr = 1, doyend = 364, nsim = 1000) %>%
        anlz_mixmeta(., yrstr = 2003, yrend = 2020) %>%
        summary() %>%
        .$coefficients

      outmt <- tibble(
        mxpval_raw = coefs1[2, 4],
        mxslos_raw = coefs1[2, 1],
        test = 'mt_raw'
      )

      out <-outmt

      return(out)

    })
  ) %>%
  select(-data, -mod) %>%
  unnest(trn)



#####################################################################################
##  Date Averaged Values
#####################################################################################

# below throws warnings but seems to run.NOTE Viewing tbtnmod takes a while to pop up.
formt<-readRDS("formt.RDS")

tbtnmod <- formt %>% 
    group_by(segment,station,param)%>% 
  nest() %>% 
  mutate(
    mod = purrr::pmap(list(station,param,data), function(station,param, data){
            cat(station, '\n')
            anlz_gam(data, trans = 'log10')
          })
  )%>%
  ungroup()

## from figs.r
modcmp <- tbtnmod %>%
  mutate(
    trn = purrr::pmap(list(station,param, mod, data), function(station,param, mod, data){

      cat(station, 'all \n')

      tomod <- data %>%
        filter(yr > 2002) %>%
        mutate(
          mo = month(date)
        )

# run meta test
      coefs1 <- anlz_metseason(mod, metfun = mean, doystr = 1, doyend = 364, nsim = 1000) %>%
        anlz_mixmeta(., yrstr = 2003, yrend = 2020) %>%
        summary() %>%
        .$coefficients

      outmt <- tibble(
        mxpval = coefs1[2, 4],
        mxslos = coefs1[2, 1],
        test = 'mt'
      )

      out <-outmt

      return(out)

    })
  ) %>%
  select(-data, -mod) %>%
  unnest(trn)



##########################################################;
##  SKT
# load file pre-wrangled for SKT trends - i.e. period assigned and median value by period
forskt<-readRDS("forskt.RDS")

# declare within year sampling frequency for autocorrelation test
tsfreq<-8

#define end year
endyr<-2020 #list.files

df_total = data.frame()
n.doit = data.frame()
datalist = list()

lvl_loc <- levels(forskt$lvl)
sta_loc <- levels(forskt$Station)
sta_loctib<-tibble(sta_loc)

for(k in 1:length(lvl_loc)){
   bylvl<-forskt%>%
    filter(lvl==lvl_loc[[k]])

## then pull a station from level;
for(j in 1:length(sta_loc)){
    TN<-bylvl%>%
    filter(Station==sta_loc[[j]])

# Make list of params within station and level
stas <- unique(TN$Param)
staname<-tibble(stas)

# make list frame for results
res <- data.frame(matrix(nrow = length(stas), ncol = 4))
colnames(res)<-c("pval","slope","pval.corr","test")

# process by param, perform SKT, and output results to res
for(i in 1:length(stas)){
  doit<-subset(TN,Param==stas[[i]])
  sk <- tibble(unlist(rkt(doit$yr,doit$value,doit$mon,,TRUE,rep="m")))
  res[i,1]<- sk[1,]
  res[i,2]<- sk[3,]
  res[i,3]<- sk[5,]
  res[i,4]<- 'sk'
  
#df_total<-rbind(df_total,res)
}

##################################################
# detect autocorrelation
#################################################

DW <- rep(NA, length(stas))
autocorr <- rep(NA, length(stas))

for(i in 1:length(stas)){
  doit<-subset(TN,Param==stas[[i]])
  doit<-na_interpolation(doit) #interpolate missing values for autocorr test only
  doit_ts<-ts(doit$value,frequency=tsfreq) #assign as timeseries with frequency interval
  fit.cr <- stl(doit_ts,s.window="period") #use residuals from deseasonal and detrended timeseries
  fit.cr2<-as.data.frame(fit.cr$time.series)#convert to dataframe to extract
  test<-serialCorrelationTest(fit.cr2$remainder, test = "AR1.mle") #this may have increased false positive due to asymptotic test
  autocorr[[i]] <-test$p.value
  DW[[i]]<-durbinWatsonTest(fit.cr2$remainder) #DW may be more appropriate for small sample size but results are different. Dont use
}

  # gather results
aucor<-data.frame(autocorr)
colnames(aucor)<-"Autocorr.p"
DWW<-data.frame(DW)
colnames(DWW)<-"DW"

## create dataframe of all results
TN_results<-cbind(staname,res,aucor,DWW) 

df_total<-rbind(df_total,TN_results)
}
}

df_total2<-cbind(modcmp_raw, modcmp, df_total)

```











W1tn<-mut%>%
  filter(station =="W1")

## this puts out a large GAM list from which you can extract fitted values, raw data, etc. 
mod<-anlz_gam(W1tn, trans = 'log10')

anlz_smooth(mod)
anlz_fit(mod)
 

 ylab <- "TN"
show_prddoy(mod, ylab = ylab)

show_prdseries(mod, ylab = ylab)

show_prdseason(mod, ylab = ylab)

show_prd3d(mod, ylab = ylab)

# works off the GAM "mod"
chg<-anlz_perchg(mod, baseyr = 2003, testyr = 2020)%>% 
    dplyr::mutate(pval = anlz_pvalformat(pval)) %>% 
    dplyr::mutate_if(is.numeric, round, 2)
  ttl <- paste0('Base: ', chg$baseval, ', Test: ', chg$testval, ', Change: ', chg$perchg, '%, ', chg$pval)

# this looks to be jsut a test between the start and end points rather than the output from mixed model stuff  
show_perchg(mod, baseyr = 2003, testyr = 2020, ylab = "Chlorophyll-a (ug/L)")

#season
metseason <- anlz_metseason(mod, metfun = mean, doystr = 1, doyend = 360, nsim = 10000)

#Uses reml instead of ml
mixed<- anlz_mixmeta(metseason, yrstr = 2003, yrend = 2020)

#extract coefficients
cm<-as.data.frame(as.matrix(summary(mixed)$coef))
#rename columns and include first column rowname
cm<-setNames(cbind(rownames(cm), cm, row.names = NULL), 
         c("Coefficient","Estimate","Stderr","Z","pval","lower95","Upper95"))



show_metseason(mod, doystr = 90, doyend = 180, yrstr = 2003, yrend = 2020, ylab = "TN (mg/L)")

show_metseason(mod, doystr = 90, doyend = 180, yrstr = NULL, yrend = NULL, ylab = "Chlorophyll-a (ug/L)")

trndseason <- anlz_trndseason(mod, doystr = 90, doyend = 180, justify = 'left', win = 5)
head(trndseason)

show_trndseason(mod, doystr = 90, doyend = 180, justify = 'left', win = 5, ylab = 'Chl. change/yr, average')

show_sumtrndseason(mod, doystr = 90, doyend = 180, justify = 'left', win = 5:15)








ready<-mut
# make lists and dataframes to store results;
df_total = data.frame()
n.doit = data.frame()
datalist = list()

lvl_loc <- levels(ready$lvl)
sta_loc <- levels(ready$station)
sta_loctib<-tibble(sta_loc)


for(k in 1:length(lvl_loc)){
   bylvl<-ready%>%
    filter(lvl==lvl_loc[[k]])


## then pull a station from level;
for(j in 1:length(sta_loc)){
    TN<-bylvl%>%
    filter(Station==sta_loc[[j]])


# Make list of params within station and level
stas <- unique(TN$Param)
staname<-tibble(stas)








# takes monthly median value
med<-mut%>%
   arrange(Station,Param,yr,mon,lvl)%>%
  group_by(Station,Param,yr,mon,lvl)%>%
  summarize(med_val=median(value,na.action=na.pass)) # allow NA's to pass

# produces count for nonmissing values
cnt<-med%>%
  filter(!is.na(med_val))%>%
  group_by(Station,Param,lvl)%>%
  summarize(c.nomiss=n(),grand_med=median(med_val,na.action=na.rm))%>%
  droplevels()

# inclusion criteria require 60 non missing values
subit<-full_join(med,cnt,by=c("Station","Param","lvl"))%>%
  filter(c.nomiss>59)%>%
  droplevels()

ready<-subit%>%
  mutate(value=med_val)%>%
  arrange(Station,Param,yr,mon,lvl)

# make lists and dataframes to store results;
df_total = data.frame()
n.doit = data.frame()
datalist = list()

lvl_loc <- levels(ready$lvl)
sta_loc <- levels(ready$Station)
sta_loctib<-tibble(sta_loc)


#####################################################
### begin analysis using a series of nested for loops
### You need to highlight entire code block enclosing
### the "END LOOPS" comment
#####################################################

#pick level first
for(k in 1:length(lvl_loc)){
   bylvl<-ready%>%
    filter(lvl==lvl_loc[[k]])


## then pull a station from level;
for(j in 1:length(sta_loc)){
    TN<-bylvl%>%
    filter(Station==sta_loc[[j]])


########################################################################
## Perform seasonal kendall trend test using rkt package and a for loop
########################################################################

# Make list of params within station and level
stas <- unique(TN$Param)
staname<-tibble(stas)

# make list frame for results
res <- data.frame(matrix(nrow = length(stas), ncol = 4))
colnames(res)<-c("pval","slope","pval.corr","tau")

# process by param, perform SKT, and output results to res
for(i in 1:length(stas)){
  doit<-subset(TN,Param==stas[[i]])
  mod <- as.data.frame(unlist(rkt(doit$yr,doit$med_val,doit$mon,,TRUE,rep="m")))
  res[i,1]<- mod[1,]
  res[i,2]<- mod[3,]
  res[i,3]<- mod[5,]
  res[i,4]<- mod[12,]

}

## returns  na's for corrected pvals due to [1 block with less than 4 points ignored]

##################################################
# detect autocorrelation
#################################################
DW <- rep(NA, length(stas))
autocorr <- rep(NA, length(stas))

for(i in 1:length(stas)){
  doit<-subset(TN,Param==stas[[i]])
  doit<-na_interpolation(doit) #interpolate missing values for autocorr test only
  doit_ts<-ts(doit$med_val,frequency=tsfreq) #assign as timeseries with frequency interval
  fit.cr <- stl(doit_ts,s.window="period") #use residuals from deseasonal and detrended timeseries
  fit.cr2<-as.data.frame(fit.cr$time.series)#convert to dataframe to extract
  test<-serialCorrelationTest(fit.cr2$remainder, test = "AR1.mle") #this may have increased false positive due to asymptotic test
  autocorr[[i]] <-test$p.value
  DW[[i]]<-durbinWatsonTest(fit.cr2$remainder) #DW may be more appropriate for small sample size but results are different. Dont use

}


# gather results
aucor<-data.frame(autocorr)
colnames(aucor)<-"Autocorr.p"

DWW<-data.frame(DW)
colnames(DWW)<-"DW"


## create dataframe of all results
TN_results<-cbind(staname,res,aucor,DWW)

########################################################################
### assign pvalue based on autocorr results
### use uncorrected p where autocorr failed to reject
### Apply Benjamini and Hochberg correction for multiple comparisons
########################################################################
TN_result2<-TN_results%>%
  mutate(corrqual=ifelse(Autocorr.p<0.05,1,0),
         padj=ifelse(corrqual==1,pval.corr,pval),
         padj=ifelse(!is.na(padj),padj,pval),
         p.FDR=p.adjust(padj,"BH"),
         raw_td  =  ifelse(padj<0.05,"Trend","No Trend"),
         final_td = ifelse(p.FDR<0.05,"Trend","No Trend"),
         final_dir = ifelse(slope<0,"Decreasing","Not"),
         final_dir = ifelse(slope>0,"Increasing",final_dir),
         final_est = ifelse(final_td=="Trend",final_dir,"Not"),
         final_stat = as.factor(sta_loc[[j]]),
         final_lvl  = as.factor(lvl_loc[[k]]),
                 )%>%
           arrange(stas)


  df_total <- rbind(df_total,TN_result2)

}

}

##############################
###   End loops            ###
##############################

### NOTE - THERE ARE CASES WHERE LESS THAN 4 DATA POINTS EXIST IN A PARTICULAR BLOCK (SAMPLE PERIOD)
###        WHICH THROWS A WARNING. IN THESE CASES THE BLOCK IS IGNORED


df_total <- arrange(df_total,stas,final_stat,final_lvl)%>%ungroup()


#add in nobs
ndoit2<-cnt%>%
  filter(c.nomiss>59)%>%
  mutate(stas = Param,final_stat = as.factor(Station),final_lvl=lvl, counts = c.nomiss)%>%
 droplevels()%>% ungroup%>%
arrange(stas,final_stat,final_lvl)


# do two joins here to get 3 datasets merged since it doesn't seem to like joining three

df_total2<-left_join(df_total,ndoit2)%>%  #,by=c(stas,final_stat)
  mutate(rel_val = slope/grand_med,
         rel_mag = ifelse(rel_val>0.10,"Large","Small"))%>%  # calculate relative magnitude of slope
  arrange(final_stat)


df_total3<-full_join(df_total2,location)

# Export to a txt file for GIS
write.table(df_total3,"PC trends full join.txt", sep=",",row.names = FALSE, col.names=TRUE)

# or use the leaflet package to plot directly from R using a ancillary program accessed via the github links below

  saveRDS(df_total3, file = "PC_trends_full_join.Rds")

# https://mikewessel.github.io/newPC/Trends.html
# https://github.com/mikewessel/newPC

























```


